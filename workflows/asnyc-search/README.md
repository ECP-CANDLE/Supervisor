# Run Asynchronous Search based hyperparameter optimization on CANDLE Benchmarks

async-search is an asynchronous iterative optimizer written in Python. It evaluates the best values of hyperparameters for CANDLE "Benchmarks" available here: `git@github.com:ECP-CANDLE/Benchmarks.git`

## Running ##

1. cd into the *~/Supervisor/workflows/async-search/test* directory
2. Specify the async-search parameters in the *cfg-prm-1.sh* file (INIT_SIZE, etc.).  
3. Specify the PROCS, queue etc. in *cfg-sys-1.sh* file
(NOTE: currently INIT_SIZE must be at least PROCS-2)
4. You will pass the MODEL_NAME, SITE, and optional experiment id arguments to *test-1.sh* file when launching:
`./test-1.sh <model_name> <machine_name> [expid]`
where `model_name` can be tc1 etc., `machine_name` can be local-as, cori, theta, titan etc. (see [NOTE](#making_changes) below on creating new SITE files.)
5. The benchmark will be run for the number of processors specified
6. Final objective function values, along with parameters, will be available in the experiments directory and also printed


## User requirements ##

What you need to install to run the workflow:

* This workflow - `git@github.com:ECP-CANDLE/Supervisor.git` .
  Clone and switch to the `master` branch. Then `cd` to `workflows/async-search`
  (the directory containing this README).
* TC1 benchmark - `git@github.com:ECP-CANDLE/Benchmarks.git` .
  Clone and switch to the `frameworks` branch.
* benchmark data -
 See the individual benchmarks README for obtaining the initial data

## Calling sequence ##

Function calls:
```
test-1.sh -> swift/workflow.sh ->

      (Async-search via EQPy)
      swift/workflow.swift <-> python/async-search.py

      (Benchmark)
      swift/workflow.swift -> obj_folder/obj_app.swift ->
      common/sh/model.sh -> common/python/model_runner.py -> 'calls Benchmark'

      (Results from Benchmark returned directly to Async-search)
      obj_folder/obj_app.swift -> python/async-search.py
```

Scheduling scripts:
```
test-1.sh -> cfg-sys-1.sh ->
      common/sh/<machine_name> - module, scheduling, langs .sh files
```
## Making Changes <a name="making_changes"></a>##

To create your own SITE files in workflows/common/sh/:
- langs-SITE.sh
- langs-app-SITE.sh
- modules-SITE.sh
- sched-SITE.sh config

copy existing ones but modify the langs-SITE.sh file to define the EQPy location (see workflows/common/sh/langs-local-as.sh for an example).

### Structure ###

The point of the script structure is that it is easy to make copy and modify the `test-\*.sh` script, and the `cfg-\*.sh` scripts.  These can be checked back into the repo for use by others.  The `test-\*.sh` script and the `cfg-\*.sh` scripts should simply contain environment variables that control how `workflow.sh` and `workflow.swift` operate.

`test-1` and `cfg-{sys,prm}-1` should be unmodified for simple testing.

### Where to check for output ###

This includes error output.

When you run the test script, you will get a message about `TURBINE_OUTPUT` .  This will be the main output directory for your run.

* On a local system, stdout/stderr for the workflow will go to your terminal.
* On a scheduled system, stdout/stderr for the workflow will go to `TURBINE_OUTPUT/output.txt`

The individual objective function (model) runs stdout/stderr go into directories of the form:

`TURBINE_OUTPUT/EXPID/run/RUNID/model.log`

where `EXPID` is the user-provided experiment ID, and `RUNID` are the various model runs generated by async-search, one per parameter set, of the form `R_I_J` where `R` is the restart number, `I` is the iteration number, and `J` is the sample within the iteration.
