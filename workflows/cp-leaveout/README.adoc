
= Challenge Problem: Leave Out

== Quick start

* On Summit, be sure to run from a compute-node writable directory +
  E.g., /gpfs/alpine/med106/scratch/$USER
* Check out Benchmarks branch loocv
** Edit uno_baseline_keras2.py to replace uno_default_model.txt +
   with uno_auc_model.txt
** Edit uno_auc_model.txt to set epochs=3 (or whatever for a shorter run)
* Check out Supervisor branch develop
* Edit test-1.sh to set data locations:
+
----
PLAN_JSON      (file: see below)
DATAFRAME_CSV  (file: see below)
BENCHMARK_DATA (full path of directory Benchmarks/Pilot1/Uno)
----

BENCHMARK_DATA must contain uno_auc_model.txt

Retrieve:
----
# The original data file
$ wget http://www.mcs.anl.gov/~wozniak/data/top21_dataframe_8x8.csv (DATAFRAME_CSV)
# The plan file (used by topN_to_uno to make a training file) (PLAN_JSON)
$ wget http://www.mcs.anl.gov/~wozniak/data/plangen_cell8-p2_drug8-p2.json
# An example training file
$ wget http://www.mcs.anl.gov/~wozniak/data/topN.uno.h5
----

=== Run

----
$ test/test-1.sh <SITE> -a
----

This will autogenerate a new experiment directory X001, X002, ...

This will run the short N=2, S=2 case.

Any additional arguments appended to the end of this command line will be passed through to workflow.swift .  See the header of workflow.swift for those options.

=== Restart

----
$ test/test-1.sh <SITE> <EXPID> -r
----

EXPID should be a previously completed experiment (e.g., X001, X002, ...)

This will modify the given EXPID in place (you may want to back it up first).

=== DB check

Quick check of DB contents after run:

----
$ ./check-db.sh <FILE.db>
----

== Bigger runs

The plan plangen_cell8-p2_drug8-p2.json supports up to N=4 children for S=3 stages.
You can run the whole file with:

----
$ test/test-1.sh <SITE> -a -N=4 -S=3
----

The -N -S arguments are passed through to workflow.swift

== Data setup

This workflow sets up the data for each run using the new model_runner feature to specify a pre_run Python module.  This is controlled by the additional hyperparameters in workflow.swift:make_json_fragment(), namely 'pre_module'.

The workflow:

. passes these hyperparameters to each obj() call,
. through model.sh,
. into model_runner.py,
. which loads the specified module data_setup (found in PYTHONPATH),
. which imports topN_to_uno,
. constructs the training H5 input file model.h5,
. and sets up symlinks to other data, the Uno cache and the uno_auc_model.txt .
. The available 'post_module' is currently unused.

== Cache

Uno tries to cache certain data which greatly speeds up training start.
This cache is specified in the JSON fragment (workflow.swift:make_json_fragment()).  It is a relative path to the run directory.  data_setup.py:pre_run() will try to create a soft link to the cache at $BENCHMARK_DATA/cache .  BENCHMARK_DATA is specified in test-*.sh.  If you want this to be a soft link to a big FS, you must create that soft link before running the workflow.

To delete the cache, simply remove the contents of the cache directory.

If you change the data set, the cache will be inconsistent and Uno will fail.  You must delete the cache so it will be rebuilt.
