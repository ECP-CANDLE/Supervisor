
:toc:

////
You can compile this locally with
$ ../docs/adoc.sh README.adoc
or just view it on GitHub.

For compatibility for the GitHub and asciidoc program,
internal links <<.>> have to be specified with headers [[.]]
////

= Supervisor Workflows

Demonstration workflows for CANDLE.

[[matrix]]
== The Matrix

This is a matrix of workflow capabilities and CANDLE benchmarks.

[options="header"]
|====
| Workflow  | Synth^1^ | AUEN | https://github.com/ECP-CANDLE/Benchmarks/tree/master/Pilot1/P1B1[P1B1] | https://github.com/ECP-CANDLE/Benchmarks/tree/master/Pilot1/P1B2[P1B2] | https://github.com/ECP-CANDLE/Benchmarks/tree/master/Pilot1/P1B3[P1B3] | https://github.com/ECP-CANDLE/Benchmarks/tree/master/Pilot2/P2B1[P2B1] | NT3
| Invoke^2^ | | <<py-keras.swift>> | | | | |
| Random^3^ | | | | | | |
| Grid^4^
| <<SimpleSweepCommandLine,SimpleSweepCLI>>  +
  <<SimpleSweepPython>> | | <<p1b1_grid>> | | | |
| https://github.com/DEAP/deap[DEAP] |
  <<GA0>> | <<AUEN_DEAP>> | | | | |
| Hyperopt  | | | <<p1b1_hyperopt>> | | | |
| https://github.com/mlr-org/mlrMBO[mlrMBO] | | | <<p1b1_mlrMBO>> | |
                                                  <<p1b3_mlrMBO>> |
                                                  <<p2b1_mlrMBO>> |
                                                  <<nt3_mlrMBO>>
|====

. _Invoke_ means that we can call this one time from Swift
. _Synth_ means some kind of synthetic task: not a real ML
. Random search
. Grid search

== Demonstrations

[[SimpleSweepCommandLine]]
=== SimpleSweepCommandLine

Demonstrates calling Swift parameter sweep over Python command line tasks.

https://github.com/CODARcode/SwiftExamples/tree/master/SimpleSweepCommandLine[CODARCode/SwiftExamples] SimpleSweepCommandLine

*Systems tested:* Local machine, http://swift-lang.github.io/swift-t/sites.html#_beagle[Beagle]

[[SimpleSweepPython]]
=== SimpleSweepPython

Demonstrates calling Swift parameter sweep over Python in-memory tasks.

http://github.com/CODARcode/SwiftExamples[CODARCode/SwiftExamples] SimpleSweepPython

*Systems tested:* Local machine, http://swift-lang.github.io/swift-t/sites.html#_beagle[Beagle]

=== SimpleSweepPyDB (WIP)

Demonstrates calling Swift parameter sweep over Python in-memory tasks plus inserts to Solr database via pysolr.

http://github.com/CODARcode/SwiftExamples[CODARCode/SwiftExamples] SimpleSweepPyDB

*Systems tested:* WIP: Local machine

[[p1b1_grid]]
=== p1b1_grid

Demonstration of P1B1 on a regular grid parameter sweep.

See https://github.com/ECP-CANDLE/Supervisor/tree/master/workflows/p1b1_grid for more details.

[[AUEN_DEAP]]
=== AUEN_DEAP

This demo runs an AUEN/Theano evolutionary algorithm.

See the https://github.com/CODARcode/SwiftExamples/tree/master/auen[README here].

*Systems tested:* http://swift-lang.github.io/swift-t/sites.html#_beagle[Beagle]

=== Simple Sweep Command Line

Simple parameter sweep that uses a Python command line task.

* https://github.com/CODARcode/SwiftExamples/tree/master/SimpleSweepCommandLine

=== Simple Sweep Python

Simple parameter sweep that uses a Python function task.

* https://github.com/CODARcode/SwiftExamples/tree/master/SimpleSweepPython

[[GA0]]
=== GA0

Genetic algorithm, difficulty zero.  Pure math objective function with DEAP optimization.

https://github.com/emews/EQ-Py/tree/master/examples/ga0

=== Older auen workflow

This is a Beagle parameter sweep over an older AUEN.

* https://github.com/CODARcode/SwiftExamples/tree/master/auen

=== auen41_ff on Cooley

*Contacts:* Wozniak and Balaprakash +
*Source:* +git@github.com:ECP-CANDLE/Supervisor.git+ http://github.com/ECP-CANDLE/Supervisor/tree/master/workflows[+/workflows/auen41_ff+] +
*Systems tested:* http://swift-lang.github.io/swift-t/sites.html#cooley_candle[Cooley]

Add this Swift/T to your +PATH+: +~wozniak/Public/sfw/x86_64/login/swift-t-conda/stc/bin+

[[py-keras.swift]]
=== py-keras.swift

This simply demonstrates that the model can be run from Swift/T +python()+.

We took the Python program https://github.com/ECP-CANDLE/Supervisor/blob/master/workflows/auen41_ff/auen41_ff.py[auen41_ff.py] and turned it into a library that can be imported and run from Swift/T.  The new function entry point is +go()+.  The program still works from the command line

https://github.com/ECP-CANDLE/Supervisor/blob/master/workflows/auen41_ff/py-keras.swift[py-keras.swift] simply loads the module +auen41_ff+ and runs +go()+.

The +go()+ function accepts the directory containing the +breast.train.csv+ and +breast.test.csv+ files.  These can be obtained on Cooley at +~wozniak/Public/data/CANDLE/auen41_ff+ .

The run script that you launch is https://github.com/ECP-CANDLE/Supervisor/blob/master/workflows/auen41_ff/py-keras-cooley.sh[py-keras-cooley.sh] .  The only non-trivial thing here is that we have to set +PYTHONHOME+ for Keras but we cannot let +qsub+ see this variable (or it will fail), so we hide it as +PH+, and send it to Swift via +swift-t -e+.

This obtains settings from https://github.com/ECP-CANDLE/Supervisor/blob/master/workflows/auen41_ff/settings.sh[settings.sh], including +QUEUE+, +PROJECT+, etc.

Output goes in numbered directories +out-NNN+.

==== Example transcript

----
$ ./py-keras-cooley.sh ~wozniak/Public/data/CANDLE/auen41_ff
TURBINE-COBALT SCRIPT
...
JOB_ID=...
... # Job runs...
TOTAL_TIME=...
# Job completed
# View output:
$ less out-001/output.txt
----

[[p1b1_hyperopt]]
=== p1b1_hyperopt

The P1B1 hyperopt workflow evaluates a modified version of the P1B1 benchmark autoencoder using hyperparameters provided by a hyperopt instance. The P1B1 code (p1b1_baseline.py) has been modified to expose a functional interface. The neural net remains the same. Currently, hyperopt minimizes the validation loss.

See https://github.com/ECP-CANDLE/Supervisor/tree/master/workflows/p1b1_hyperopt for more details.

[[p1b1_mlrMBO]]
=== p1b1_mlrMBO

The P1B1 mlrMBO workflow evaluates a modified version of the P1B1 benchmark autoencoder using hyperparameters provided by a mlrMBO instance. The P1B1 code (p1b1_baseline.py) has been modified to expose a functional interface. The neural net remains the same. Currently, mlrMBO minimizes the validation loss.

See https://github.com/ECP-CANDLE/Supervisor/tree/master/workflows/p1b1_mlrMBO for more details.

[[p1b3_mlrMBO]]
=== p1b3_mlrMBO

The P1B3 mlrMBO workflow evaluates the P1B3 benchmark
using hyperparameters provided by a mlrMBO instance. mlrMBO
minimizes the validation loss.

See https://github.com/ECP-CANDLE/Supervisor/tree/master/workflows/p1b3_mlrMBO for more details.

*Systems tested:* http://www.nersc.gov/users/computational-systems/cori[Cori]

[[p2b1_mlrMBO]]
=== p2b1_mlrMBO

The P2B1 mlrMBO workflow evaluates the P2B1 benchmark
using hyperparameters provided by a mlrMBO instance. mlrMBO
minimizes the validation loss (???).

See https://github.com/ECP-CANDLE/Supervisor/tree/master/workflows/p2b1_mlrMBO for more details.

*Systems tested:* http://www.nersc.gov/users/computational-systems/cori[Cori]


[[nt3_mlrMBO]]
=== nt3_mlrMBO

See https://github.com/ECP-CANDLE/Supervisor/tree/master/workflows/nt3_mlrMBO for more details.

== Objective function guide

In CANDLE, *objective functions* are the calls to the machine learning (ML) models.  They are functions that accept some parameter tuple describing how the model will be run, and return some value, such as a loss.  Typical CANDLE workflows optimize the return value in some parameter space using some model exploration algorithm (ME).

This documents how to read existing objective functions and develop new ones.

=== Swift/T leaf functions

Objective functions are implemented as Swift/T leaf functions, which are http://swift-lang.github.io/swift-t/guide.html#leaf_functions[described here].  In short, leaf functions are opaque to Swift.  For the purposes of CANDLE, a leaf function is a command line program or a call to evaluate a string of Python code in-memory.  Normally, Swift/T is free to evaluate leaf functions anywhere in the system (load balancing) in any order (as long as all input data is ready).

=== Command-line programs

A typical command line invocation is here:

https://github.com/ECP-CANDLE/Supervisor/blob/3e53ec93ba5ad79c114a96287f2d280a8e93ad8a/workflows/p3b1_mlrMBO/swift/ai_workflow3.swift#L83[P3B1 mlrMBO: ai_workflow3.swift]
----
(string obj_result) obj(string params, string iter_indiv_id) {
  string outdir = "%s/run_%s" % (turbine_output, iter_indiv_id);
  file out <"%s/out.txt" % outdir>;
  file err <"%s/err.txt" % outdir>;

  (out,err) = run_model(model_script, params, outdir, iter_indiv_id) =>
  string result_file = "%s/result.txt" % outdir;
  obj_result = get_results(result_file);
  printf(obj_result);
}
----

+obj()+ is an objective function that takes parameters and returns a string to Swift.  The parameters (+params+) are produced by the ME and are encoded as a JSON fragment.  You can simply print them out in Swift (via +printf()+) to see them.  A unique identifier +iter_indiv_id+ is also provided and used to create a unique output directory for +out.txt+ and +err.txt+.  The model is actually executed in +run_model()+, described below.  Then, its results are obtained by +get_results()+, and also logged to +stdout+ (via +printf()+).

https://github.com/ECP-CANDLE/Supervisor/blob/3e53ec93ba5ad79c114a96287f2d280a8e93ad8a/workflows/p3b1_mlrMBO/swift/ai_workflow3.swift#L35[P3B1 mlrMBO: ai_workflow3.swift]
----
app (file out, file err) run_model (file shfile, string params_string, string instance, string run_id)
{
    "bash" shfile params_string emews_root instance FRAMEWORK exp_id run_id benchmark_timeout @stdout=out @stderr=err;
}

This is a Swift +app+ function.  Its body is a command line, populated with the input and output arguments.  Thus, it runs +bash+ on a given script with the parameters, as specified in +obj()+.  Some of the variables referenced in the body are Swift global variables.  The special syntax +@stdout+, +@stderr+ capture those streams respectively.

=== In-memory Python functions



== Works in progress

=== Oversample

Some kind of preliminary test.

https://github.com/CODARcode/SwiftExamples/tree/master/oversample
