
= Mini Challenge Problem

The Mini Challenge Problem has 4 distinct stages: 1) preprocessing feature selection; 2) HPO optimization using mlrMBO; 3) additional
training of the best models identified in 2; and 4) performing inference
on the trained models from 3. 

==== Feature Selection

Rather than using all of the ~17,000 genes for prediction of drug response for Uno, we used the COXEN approach (see the _xcorr_ README) to select genes for building the prediction model in a scenario of model adaptation between studies/datasets. The COXEN approach performs statistical analysis to identify predictive and generalizable genes for prediction. We have five public cancer cell line drug response datasets, i.e., NCI-60, CTRP, GDSC, CCLE, and gCSI. These five studies may have common cancer cell lines and drugs included in their experiments, but each of them may also include unique cell lines, drugs, or combinations of the two. Thus, the COXEN approach targeting genes that are both predictive and generalizable can be helpful in the model adaptation between studies. The approach produces _cross correleated feature set_ files
that are used as input to the model. A preprocessed data file will have a file name like `CTRP_CCLE_2000_1000_features.txt`
where the two studies are `CTRP` and `CCLE`, and the cross correlation 
coefficients are 2000 and 1000

==== HPO using mlrMBO

The HPO workflow runs N number of concurrent mlrMBO hyperparameters optimizations (HPO) of the
Uno benchmark using the cross correlated feature sets as input. A
mlrMBO instance will be run for each cross correlated feature set.


==== Further Training

The further training workflow selects N number of the best models from those trained
in the HPO workflow. An input file contains the parameters for each model run (one per row). The workflow
reads this file and launches the model runs with as much concurrency is available.


==== Inference

The inference workflow run inference on the trained models from the further training workflow. It is similar to the further training workflow, but the
parameters in the input file are those relevant for performing model inference rather than training.

== Requirements

=== Code

* The workflows. Clone git@github.com:ECP-CANDLE/Supervisor.git and switch to the develop branch. The
workflows code is in `Supervisor/workflows/cp1`.
* The Uno benchmark. Clone git@github.com:ECP-CANDLE/Benchmarks.git and switch to the
develop branch. The Uno benchmarks is in `Benchmarks/Pilot1/Uno`.

=== Data

*Download the raw Uno feature data to the `Benchmarks/Data/Pilot1` directory.*
Note the data is quite large so, depending on the file system and machine, a better 
choice may be to download and symlink to `Benchmarks/Data/Pilot1`.

----
$ cd Benchmarks
$ mkdir -p Data/Pilot1
$ wget -r -nd --no-parent -A '*' http://ftp.mcs.anl.gov/pub/candle/public/benchmarks/Pilot1/combo/
----


*Download the pregenerated cross correlated feature files.*

----
$ wget http://www.mcs.anl.gov/~wozniak/candle-cp1-data.tgz
$ tar xfz candle-cp1-data.tgz
----

*Generate the input data frames from the raw data and the feature files.*
Uno runs much faster with these as input rather than the raw data. 
A train and test data frame needs to be created for each cross correlated feature file. 

For example, to generate the data associated with the `CTRP_CCLE_2000_1000.txt` feature file:

----
$ cd Benchmarks/Pilot1/Uno
$ python python uno_baseline_keras2.py --train_sources CTRP --cell_feature_subset_path CTRP_CCLE_2000_1000_features.txt --no_feature_source True --no_response_source True --preprocess_rnaseq combat --export_data CTRP_CCLE_2000_1000_train.h5
$ python uno_baseline_keras2.py --train_sources CCLE --cell_feature_subset_path CTRP_CCLE_2000_1000_features.txt --no_feature_source True --no_response_source True --preprocess_rnaseq combat --export_data CTRP_CCLE_2000_1000_test.h5
----

Note that in the train data creation the train_source is CTRP and in the test data 
creation the train source is CCLE. The creation of the training and test data can be 
time consuming for the larger datasets. 


== Running the Workflows

=== HPO Workflow

. Edit `Supervisor/workflows/cp1/data/studies1.txt` and `Supervisor/workflows/cp1/data/studies2.txt`. 
These two study files specify the cross correlation between studies where each study in `studies1.txt` 
is cross correlated with each study in `studies2.txt` except where they are the same. Add or remove 
(or comment out) study names in these files to omit that cross-correlation from the HPO instances.
For example, if study1 contains CTRP and study2 contains CCLE and GDSC, then the workflow
will run two HPOs: one for the CTRP_CCLE and one for the CTRP_GDSC cross-correlations.

. Create a set of _cfg_ and _test_ scripts for an experiment run by
copying an existing set, i.e., `cfg-prm-1.sh`, `cfg-sys-1.sh`, and `test-1.sh`
from the `test` directory. Be sure to update the lines in your `test-N.sh` that 
export the `cfg-prm-N.sh`, and `cfg-sys-N.sh` scripts to point to your _cfg_ scripts.
Namely,
+
----
# Select configurations
export CFG_SYS=$THIS/cfg-sys-N.sh
export CFG_PRM=$THIS/cfg-prm-N.sh
----
+
See the comments in `cfg-prm-1.sh`, and `cfg-sys-1.sh` for additional
information on the various configuration parameters and how to edit them.

. Launch the run using your `test-N.sh` script, passing SITE, and optional 
experiment id as arguments (e.g., `./test-10.sh <site> [expid]`) where 
site can be one of local, cori, theta, summit etc.

All the output from running the workflow will appear in a directory named with the
experiment id, either your specified one or the auto-generated one (e.g. X001). Each Uno
model run launched by the mlrMBO instances runs in own directory: 
`exp_id/run/W_X_Y_Z` where _W_ is the id of the mlrMBO instance that launched the run, _X_ is the restart number 
(almost always 1 here), _Y_ is the iteration of the mlrMBO instance, and _Z_ is the id of the hyper parameter set
produced by mlrMBO instance _W_ and with which Uno was launched.

A summary of each Uno run, organized by mlrMBO instance and iteration, will be 
output in `exp_id/hpo_log/X_Y_hpo_runs.txt` where _X_ is the mlrMBO instance
id, and _Y_ is the mlrMBO instance's iteration. Each row of this hpo log output
contains info for a single Uno run and has the following format:

`row_index | run_id | hpo parameters | run directory | timestamp | run val loss`

where the `|` character is the delimiter.

==== Associated Files

* Configuration and launch scripts in `test/` (e.g. `cfg-prm-1.sh`, `test-1.sh`, etc.)
* `swift/workflow.swift` - swift file that executes the workflow
* `swift/workflow.sh` - launch script for the swift file. This script is 
configured and launched from the scripts in `test/`.


=== Further Training Workflow (AKA the UPF workflow)

. Select N number of models from those produced by each HPO instance and 
create the input parameter file (i.e., the "upf" file). The hpo_log results
from the HPO workflow can be used for this. The "Combine HPO logs files 
adding hpo_id and iteration" code in `scripts/plots.R` is an example of how those
logs can be concatenated together while adding the hpo_id and iteration as
column values. The python jupyter notebook `script/cp1_scripts.ipynb` contains
example code for selecting the top 10 models for each HPO from the combined HPO logs
and creating the parameter file for running the selected models. Each row
in the upf file contains the hyperparameters for an Uno run in JSON format.

+
----
{"activation": "relu", "dense": "2000 2000 2000", "dense_feature_layers": "1000 1000 1000 1000 1000", "drop": 0.2, "optimizer": "adamax", "residual": 0, "epochs": 100, "batch_size": 6144, "train_sources": "CTRP", "preprocess_rnaseq": "combat", "gpus": "0 1 2 3 4 5", "cell_feature_subset_path": "/autofs/nccs-svm1_proj/med106/ncollier/repos/Supervisor/workflows/cp1/xcorr_data/CTRP_GDSC_2000_1000_features.txt", "use_exported_data": "/autofs/nccs-svm1_proj/med106/ncollier/repos/Supervisor/workflows/cp1/cache/CTRP_GDSC_2000_1000.h5", "warmup_lr": true, "reduce_lr": true, "no_feature_source": true, "no_response_source": true, "cp": true}
----

. Create a set of _cfg_ and _test_ scripts for an experiment run by
copying an existing set, i.e., `cfg-prm-1.sh`, `cfg-sys-1.sh`, and `test-1.sh`
from the `test_upf/` directory. Note this is *NOT* the `test/` directory. Be sure 
to update the lines in your `test-N.sh` that 
export the `cfg-prm-N.sh`, and `cfg-sys-N.sh` scripts to point to your scripts.
Namely,
+
----
# Select configurations
export CFG_SYS=$THIS/cfg-sys-N.sh
export CFG_PRM=$THIS/cfg-prm-N.sh
----
+
See the comments in `cfg-prm-1.sh`, and `cfg-sys-1.sh` for additional
information on the various configuration parameters and how to edit them.

. Launch the run using your `test-N.sh` script, passing SITE, 
and optional experiment id as arguments (e.g., `./test-10.sh <site> [expid]`) where 
site can be one of local, cori, theta, summit etc.

All the output from running the workflow will appear in a directory named with the
experiment id, either your specified one or the auto-generated one (e.g., X001). Each Uno
model run launched by the workflow runs in own directory: 
`exp_id/run/X` where _X_ is the id of the run and corresponds to the index 
of the line of input data that was used for that run, that is, `run/0` contains 
the output for the run that ran with the 1st line from the upf input 
file, `run/1` for the second line and so on.

In addition, `inputs.txt` and `results.txt` files are also created.
The first contains the parameters used for each run and the second final val loss for each run.

==== Associated Files

* Configuration and launch scripts in `test_upf/` (e.g., `cfg-prm-1.sh`, `test-1.sh`, etc.)
* `swift/upf_workflow.swift` - swift file that executes the workflow
* `swift/upf_workflow.sh` - launch script for the swift file. This script is 
configured and launched from the scripts in `test_upf/`.


=== Inference

. Create the inference parameter file. Each line of the inference parameter 
file contains the HPO parameters for a single inference run in csv format with
the following columns

+
`test data,directory of the trained model,run label`
+
For example,
`CTRP_GDSC_2000_1000_test.h5,/gpfs/alpine/med106/scratch/ncollier/experiments/full_training_2/run/0/,CTRP_GDSC_2000_1000`
+
The test data is part of the data generated as part of the data requirments (see above), 
and found in the so-called CACHE_DIR directory as defined in the `cfg-prm-N.sh` files. The 
"directory of the trained model" is a directory that contains a model trained in the further
 training workflow. The run label can be an informative label for the run.  The python jupyter 
 notebook `script/cp1_scripts.ipynb` has some sample code for creating this parameter file.

. Create a set of _cfg_ and _test_ scripts for an experiment run by
copying an existing set, i.e., `cfg-prm-1.sh`, `cfg-sys-1.sh`, and `test-1.sh`
from the `test_infer/` directory. Note this is *NOT* the `test/` directory. 
Be sure to update the lines in your `test-N.sh` that 
export the `cfg-prm-N.sh`, and `cfg-sys-N.sh` scripts to point to your _cfg_ scripts.
Namely,
+
----
# Select configurations
export CFG_SYS=$THIS/cfg-sys-N.sh
export CFG_PRM=$THIS/cfg-prm-N.sh
----
+
See the comments in `test_infer/cfg-prm-1.sh`, and `test_infer/cfg-sys-1.sh` for additional
information on the various configuration parameters and how to edit them.

. Edit `sh/infer.sh`, if necessary. `infer.sh` is used to run the Uno benchmark's
`uno_infer.py` python script. Lines 31-33 can be uncommented and edited
to create multiple copies of the input data to avoid IO contention. If
this is unnecessary, then the `infer.sh` should not need to be changed.

. Launch the run using your `test-N.sh` script, passing SITE, and optional experiment id 
as arguments (e.g., `./test-10.sh <site> [expid]`) where 
site can be one of local, cori, theta, summit etc.

All the output from running the workflow will appear in a directory named with the
experiment id, either your specified one or the auto-generated one. Each Uno
model inference run launched by the workflow runs in its own directory: 
`exp_id/run/X` where _X_ is the id of the run and corresponds to the index of the 
line of input data that was used for that run. So, `run/0` contains 
the output for the run that ran with the 1st line from the input 
file, `run/1` for the second line and so on. Each inference run will 
produce an `uno_pred.all.tsv` and an `uno_pred.tsv` file. The first contains
the predictions for each feature and the second is an aggregate view 
of the first. Additionally a `log.txt` file is created in the experiment directory
that contains the name of the data input file, the model, the output directory, 
and number of predictions performed for each inference run.

==== Associated Files

* Configuration and launch scripts in `test_infer/` (e.g. `cfg-prm-1.sh`, `test-1.sh`, etc.)
* `swift/infer_workflow.swift` - swift file that executes the workflow
* `swift/infer_workflow.sh` - launch script for the swift file. This script is 
configured and launched from the scripts in `test_infer/`.
* `sh/infer.sh` - script used to launch the Uno benchmark's `uno_infer.py` to perform the actual inference.


== Additional Notes

=== Running on Summit

Summit's project directories (e.g. /ccs/proj/med106/) are not writable from the compute nodes.
The workflow needs the following directories in Supervisor/workflows/cp1 to be writable.

* cache
* experiments
* xcorr_data

One solution is to symlink them from a writable location such as /gpfs/alpine/med106/scratch.
For example,

----
$ pwd
/ccs/proj/med106/ncollier/repos/Supervisor/workflows/cp1
$ ls -l
lrwxrwxrwx 1 ncollier ncollier   46 Mar  1 10:35 cache -> /gpfs/alpine/med106/scratch/ncollier/uno_cache
lrwxrwxrwx 1 ncollier ncollier   48 Feb 28 16:51 experiments -> /gpfs/alpine/med106/scratch/ncollier/experiments
lrwxrwxrwx 1 ncollier ncollier   47 Mar  1 12:38 xcorr_data -> /gpfs/alpine/med106/scratch/ncollier/xcorr_data
----
