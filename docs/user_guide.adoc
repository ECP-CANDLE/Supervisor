= CANDLE Library User Guide

The CANDLE library enables users run their own deep learning code in high-performance computers that CANDLE supports. With the current version of CANDLE library, users will be able to run a hyperparameter optimization task (mlrMBO workflow) or a parallel execution task (upf workflow). In order to run those workflows, users are required to implement a class and methods (will be explained in section 1) and follow some procedures (see section 2) for each workflow. This user guide will provide an overview of structure and explanation of parameters or variables as needed.

= How to write CANDLE compliant deep learning code

== Minimum requirements
The CANDLE requires you to create a class and implement two methods. The base class is designed to provides common parameters such as `batch_size`, `epoch`, etc, and enables you register additional parameters as needed. In addition to that, you can mandate some parameters, if needed.

=== Base Class
The CANDLE library provides a base class (`candle.Benchmark`), which you will extend to config your parameters. If you don't have any additional parameters to define, you can use this code as is (see https://github.com/ECP-CANDLE/Candle/blob/library/examples/mnist/mnist.py[MNIST example]).
[source,python]
----
# mnist.py
additional_definitions = None
required = None

class MNIST(candle.Benchmark): // <1>
    def set_locals(self):
        if required is not None:
            self.required = set(required)
        if additional_definitions is not None:
            self.additional_definitions = additional_definitions
----
<1> Create a new class by extending `candle.Benchmark`

==== Additional Parameters
You can add a parameter like https://github.com/ECP-CANDLE/Benchmarks/blob/release_01/Pilot1/TC1/tc1.py#L16-L58[this example]. The example illustrates how to define an integer type parameter, `pool` with minimum description.

----
additional_definitions = [{
    'name':'pool', // <1>
    'nargs':'+', // <2>
    'type': int, // <3>
    'help':'network structure of shared layer' // <4>
},
]
----
<1> required. Name of parameter.
<2> optional. The number of command-line arguments.
<3> required. The type to which the command-line arguments should be converted.
<4> optional. A brief description of what the argument does.

==== Mandatory parameters
If you like to mandatory some common parameters, you will need to define `required` array and pass it to the definition of your class.
----
required = [
    'data_url',
    'epochs',
    'batch_size',
    'pool',
    'save'
]
----

==== Thread Optimization
Some high-performance computing machines like `Theta`, the performance will be greatly improved if we let CANDLE handles threads. So, it is generally recommended to have code like line 14 to 21 in https://github.com/ECP-CANDLE/Candle/blob/library/examples/mnist/mnist.py#L9-L16[this example]
----
# thread optimization
import os
from keras import backend as K
if K.backend() == 'tensorflow' and 'NUM_INTRA_THREADS' in os.environ:
    import tensorflow as tf
    sess = tf.Session(config=tf.ConfigProto(inter_op_parallelism_threads=int(os.environ['NUM_INTER_THREADS']),
                                            intra_op_parallelism_threads=int(os.environ['NUM_INTRA_THREADS'])))
    K.set_session(sess)
----


=== Initialize_parameters Method
In `initialize_parameters` method, we will instantiate the base class, and finally build argument parser to recognize your customized parameters in addition to the default parameters ( `default_utils.initialize_parameters()`). The `initialize_parameters` should return a python dictionary, which will be accessed in `run(Dict)` method.

[source,python]
----
# this is a part of mnist_mlp_candle.py

import mnist
import candle_keras as candle

def initialize_parameters():
    mnist_common = mnist.MNIST(mnist.file_path,
        'mnist_params.txt', // <4>
        'keras',
        prog='mnist_mlp',
        desc='MNIST example'
    )  // <1>

    # Initialize parameters
    gParameters = default_utils.initialize_parameters(mnist_common) // <2>
    ..

    return gParameters   // <3>
----
<1> instantiate base class
<2> build argument parser
<3> this method should return python dictionary
<4> a file that contains default values for the given parameters. See below for example.

----
[Global_Params]
epochs=20
batch_size=128
activation='relu'
optimizer='rmsprop'
----

When parameters are not overwritten by workflows, these values will be used.

=== Run Method
You can place your deep learning code in `run(Dict)` method. Your parameters are accessible like `gParameters['batch_size']`.

We have an https://github.com/ECP-CANDLE/Candle/blob/library/examples/mnist/mnist_mlp_candle.py[example], that converted a simple MNIST neural net `mnist_mlp.py` provided by https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py[Keras Team] into CANDLE compliant form. In this example, you will see how the actual neural network was transplanted in `run` method.

Finally, the `run()` returns keras history object. This can be omitted for upf workflow, but required for HPO workflow.

[source,python]
----
# this is a part of mnist_mlp_candle.py

def run(gParameters): // <1>
    ##########################################
    # Your DL start here. See mnist_mlp.py   #
    ##########################################

    ...

    batch_size = gParameters['batch_size']
    epochs = gParameters['epochs']

    ...

    model.compile(loss='categorical_crossentropy',
                optimizer=optimizer,
                metrics=['accuracy'])

    history = model.fit(x_train, y_train,
                        batch_size=batch_size,
                        epochs=epochs,
                        verbose=1,
                        validation_data=(x_test, y_test))
    ##########################################
    # End of mnist_mlp.py ####################
    ##########################################
  return history  // <2>

----
<1> run method receives parameter dictionary
<2> returns keras history object or None*

* The mlrMBO workflow requires to return keras history object so that the workflow can evaluate the model. The upf workflow does not have the evaluation process, so you can return `None`.


= How to run CANDLE compliant code in Theta
As mentioned above, we offer two different workflows in CANDLE:
Unrolled Parameter File (UPF) and Hyper Parameter Optimization (HPO).
The UPF workflow allows you to run parallel multi-node executions with different parameters,
while HPO workflow evaluates the best value of hyperparameters based on mlrMBO algorithm.


== Running UPF on Theta
This tutorial, we will execute mnist example rewritten for CANDLE. The source code is available on https://github.com/ECP-CANDLE/Candle/tree/library/examples/mnist[CANDLE github repo].

Step 1. Create directory and checkout Supervisor & Candle repos
[source,bash]
----
$ mkdir candle_tutorial
$ cd candle_tutorial
$ git clone -b master https://github.com/ECP-CANDLE/Supervisor.git
$ git clone -b library https://github.com/ECP-CANDLE/Candle.git
----

Step 2. Move to upf workflow directory
----
$ cd Supervisor/workflow/upf
----

Step 3. Set Env variables. In `test/cfg-sys-1.sh`,
you will need to set `BENCHMARK_DIR` to point the directory that your script locates, and
`MODEL_PYTHON_SCRIPT` to name the script you want to run
----
BENCHMARK_DIR=/home/hsyoo/candle_tutorial/Candle/examples/mnist <1>
MODEL_PYTHON_SCRIPT=mnist_mlp_candle <2>
----
<1> This location should reflect your environment
<2> Note this requires filename without extension (such as .py)

Step 4. Set execution plan. Check `test/upf-1.txt` for parameter configuration and modify as needed.
This file contains multiple number of JSON documents. Each JSON document will contain the command line parameters.
For example,
----
{"id": "test0", "epochs": 10}
{"id": "test1", "epochs": 20}
----
This will invoke two instances, which will run 10 epochs and 20 epochs respectively.

Step 5. Submit your job. You will need to set `QUEUE`, `PROJECT`, `PROCS`, and `WALLTIME`.
You can configure those in `cfg-sys-1.sh` (see Step 3), set as env variables, or you can provide in your command line (see below).
----
$ export QUEUE=debug-cache-quad
$ export PROJECT=myproject
$ export PROCS=3
$ export WALLTIME=00:10:00

$ ./test/upf-1.sh theta

// or 

$ QUEUE=debug-cache-quad PROJECT=myproject PROCS=3 WALLTIME=00:10:00 ./test/upf-1.sh theta
----

* `QUEUE` refers to the system queue name. The Theta machine has `default`, `debug-flat-quad`, and `debug-cache-quad`. For more information, please check https://www.alcf.anl.gov/user-guides/job-scheduling-policy-xc40-systems#queues
* `PROJECT` refers to your allocated project name. Please check https://www.alcf.anl.gov/user-guides/allocations, for more detail.
* `PROCS` is a number of nodes. We recommend adding extra 1 node in addition to the number of executions in your plan. In this example, we set 3 (1 + 2).
* `WALLTIME` refers to computing time you are requesting for individual node. The production queues are limited by policy. Check https://www.alcf.anl.gov/user-guides/job-scheduling-policy-xc40-systems#queues for more deail.


Step 6. Check queue status
----
$ qstat -u user_name -f
----

Step 7. Reviewing result files
After the job is completed, the results files are available in experiments directory (Supervisor/workflow/upf/experiments).
For example, `/home/hsyoo/candle_tutorial/Supervisor/workflows/upf/experiments/X000` will contains files like below,
----
-rw-r--r-- 1 hsyoo cobalt  2411 Aug 17 19:13 262775.cobaltlog
-rw-r--r-- 1 hsyoo users   1179 Aug 17 18:55 cfg-sys-1.sh <3>
-rw-r--r-- 1 hsyoo users      7 Aug 17 18:55 jobid.txt
-rw-r--r-- 1 hsyoo users   3310 Aug 17 19:13 output.txt <1>
drwxr-xr-x 4 hsyoo users    512 Aug 17 19:07 run <2>
-rw------- 1 hsyoo users  10863 Aug 17 18:55 swift-t-workflow.8X4.tic
-rw-r--r-- 1 hsyoo users    677 Aug 17 18:55 turbine.log
-rwxr--r-- 1 hsyoo users   5103 Aug 17 18:55 turbine-theta.sh
-rw-r--r-- 1 hsyoo users     60 Aug 17 18:55 upf-1.txt <3>
-rw-r--r-- 1 hsyoo users   4559 Aug 17 18:55 workflow.sh.log

hsyoo@thetalogin4:~/candle_tutorial/Supervisor/workflows/upf/experiments/X000> ls -al run/ <2>
total 2
drwxr-xr-x 4 hsyoo users  512 Aug 17 19:07 .
drwxr-xr-x 3 hsyoo users 1024 Aug 17 20:33 ..
drwxr-xr-x 3 hsyoo users  512 Aug 17 20:34 test0
drwxr-xr-x 3 hsyoo users  512 Aug 17 19:13 test1

hsyoo@thetalogin4:~/candle_tutorial/Supervisor/workflows/upf/experiments/X000> cat run/test0/model.log <4>
... many lines omitted ...
Epoch 10/10
60000/60000 [==============================] - 12s - loss: 4.3824 - acc: 0.7253 - val_loss: 2.1082 - val_acc: 0.8671
('Test loss:', 2.1082268813190574)
('Test accuracy:', 0.86709999999999998)
result: 2.10822688904
----
<1> `output.txt` contains stdout and stderr of this experiment. This is helpful to debug errors.
<2> `run` directory contains the output files. You will see two directories that are corresponding the IDs configured in upf-1.txt
<3> a copy of configuration files are available so that you can trace what were passed to this experiment.
<4> stdout of test0. After 10 epoches, validation loss was 2.1082. 


== Running mlrMBO based Hyperparameters Optimization (HPO) on Theta

Step 1. Create directory and checkout Supervisor & Candle repos.
You can skip this step if you already have done in previous section.
----
$ mkdir candle_tutorial
$ cd candle_tutorial
$ git clone -b master https://github.com/ECP-CANDLE/Supervisor.git
$ git clone -b library https://github.com/ECP-CANDLE/Candle.git
----

Step 2. Move to mlrMBO workflow directory
----
$ cd Supervisor/workflow/mlrMBO
----

Step 3. Set Env variables. In `test/cfg-sys-1.sh`,
you will need to set `BENCHMARK_DIR` to point the directory that your script locates, and
`MODEL_PYTHON_SCRIPT` to name the script you want to run
----
BENCHMARK_DIR=/home/hsyoo/candle_tutorial/Candle/examples/mnist <1>
MODEL_PYTHON_SCRIPT=mnist_mlp_candle <2>
----
<1> This location should reflect your environment
<2> Note this requires filename without extension (such as .py)

Step 4. Config hyper parameters. In this step, we are configuring parameter sets, which we will iteratively evaluate.
For example, you can create `workflow/data/mnist.R` as below.
----
param.set <- makeParamSet(
  makeDiscreteParam("batch_size", values=c(32, 64, 128, 256, 512)),
  makeDiscreteParam("activation", values=c("relu", "sigmoid", "tanh")),
  makeDiscreteParam("optimizer", values=c("adam", "sgd", "rmsprop")),
  makeIntegerParam("epochs", lower=20, upper=20)
)
----
This file should locate under your Supervisor installation. For this tutorial, it is ```/home/hsyoo/candle_tutorial/Supervisor/workflows/mlrMBO/data```, but again, this should reflect your environment.

In this example, we are varying four paramters, `batch_size`, `activation`, `optimizer`, `epochs`. For `batch size`, we are trying out 32, 64, 128, 256, and 512. For `activation` method, we are exploring `relu`, `sigmoid`, and `tanh`, and so on. Entire parameter space will be 45 (5 x 3 x 3 x 1).

After creating this file, we need to point this file.
----
$ export PARAM_SET_FILE=mnist.R
----

Step 5. Submit your job.

----
$ ./test/test-1.sh mnist theta
----

The first argument is MODEL_NAME. If the name is registered in `test/cfg-prm-1.sh`, it will use the pre-configured parameter file.
Otherwise, CANDLE will use `PARAM_SET_FILE` we configured in step 4.

You can specify the HPO search strategy. As you can see in `test/cfg-prm-1.sh`, you are able to config `PROPOSE_POINTS`, `MAX_CONCURRENT_EVALUATIONS`, `MAX_ITERATIONS`, `MAX_BUDGE`, `DESIGN_SIZE`.

* `DESIGN_SIZE` is a number of parameter sets that will evaluate at the beginning of HPO search. In this example, CANDLE will select random 10 parameter sets out of 45 (see Step 4, for break downs).
* `MAX_ITERATIONS` is a number of iterations.
* `PROPOSE_POINTS` is a number of parameter sets that CANDLE will evaluate in each iteration. So, if `MAX_ITERATION=3` and `PROPOSE_POINTS=5`, CANDLE will be ended up evaluating 25 params (10 + 3 x 5).
* `MAX_BUDGET` should be greater than total evaluations. In this example, 45.

